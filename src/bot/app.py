import os
import textwrap
from typing import Sequence

import streamlit as st
import torch
from dotenv import load_dotenv
from langchain.chains.combine_documents.base import BaseCombineDocumentsChain
from langchain.chains.summarize import load_summarize_chain
from langchain.schema import StrOutputParser
from langchain.text_splitter import CharacterTextSplitter
from langchain_community.vectorstores import FAISS
from langchain_core.documents import Document
from langchain_core.prompts import ChatPromptTemplate
from langchain_core.runnables import RunnablePassthrough
from langchain_huggingface import HuggingFaceEmbeddings as HFEmbedder
from langchain_ollama.llms import OllamaLLM

from bot.models import NewsArticle
from bot.services import EXAMPLES_CLS_TOPIC, DatabaseManager, TopicClassifier, load_config, setup_logging

torch.classes.__path__ = []

PROMPT = ChatPromptTemplate.from_template(
    textwrap.dedent(
        """
        –ü—Ä–µ–¥—Å—Ç–∞–≤—å —á—Ç–æ —Ç—ã —Å—Ç–µ–Ω–¥–∞–ø-–∫–æ–º–∏–∫ –∏ —Ç—ã –æ—á–µ–Ω—å —Å–º–µ—à–Ω–æ –∫–æ–º–º–µ–Ω—Ç–∏—Ä—É–µ—à—å –ø–æ—Å–ª–µ–¥–Ω–∏–µ –Ω–æ–≤–æ—Å—Ç–∏. –ó–∞ —Ö–æ—Ä–æ—à–∏–µ —à—É—Ç–∫–∏ —Ç–µ–±–µ –ø–ª–∞—Ç—è—Ç –æ—á–µ–Ω—å —Ö–æ—Ä–æ—à–∏–µ –¥–µ–Ω—å–≥–∏.
        –¢–≤–æ—è –∑–∞–¥–∞—á–∞ –ø—Ä–∏–¥—É–º–∞—Ç—å —Ö–æ—Ä–æ—à—É—é —à—É—Ç–∫—É, –æ–ø–∏—Ä–∞—è—Å—å –Ω–∞ –∫–æ–Ω—Ç–µ–∫—Å—Ç –ø–æ —Ç–µ–º–µ, –∫–æ—Ç–æ—Ä—É—é —è —Ç–µ–±–µ –¥–∞–º. –¢–≤–æ—è —à—É—Ç–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Ç–æ–Ω–∫–æ–π –∏ –æ—Å—Ç—Ä–æ—É–º–Ω–æ–π.
        –®—É—Ç–∫–∞ –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å –≤ 1-2 –ø—Ä–µ–¥–ª–æ–∂–µ–Ω–∏—è, –¥–æ–ª–∂–Ω–∞ –±—ã—Ç—å —Å–º–µ—à–Ω–æ–π –∏ —Ç–æ–Ω–∫–æ–π –∏ –æ–ø–∏—Ä–∞—Ç—å—Å—è –Ω–∞ –¥–∞–Ω–Ω—ã–π –∫–æ–Ω—Ç–µ–∫—Å—Ç!

        –ö–æ–Ω—Ç–µ–∫—Å—Ç:
        {context}

        –¢–µ–º–∞: {question}

        –®—É—Ç–∫–∞:
    """
    )
)


def format_docs(docs: list[Document]) -> str:
    return "\n\n".join([d.page_content for d in docs])


def summarize_news(docs: list[Document], chain: BaseCombineDocumentsChain) -> list[str]:
    """–°—É–º–º–∞—Ä–∏–∑–∞—Ü–∏—è –Ω–æ–≤–æ—Å—Ç–µ–π —Å –ø–æ–º–æ—â—å—é LLM."""
    summaries = []
    for doc in docs:
        summary = chain.invoke([doc])
        summaries.append(f"üìå {summary['output_text']}")
    return summaries


def generate_response(
    llm: OllamaLLM,
    news: Sequence[NewsArticle],
    embedder: HFEmbedder,
    summarize_chain: BaseCombineDocumentsChain,
    splitter: CharacterTextSplitter,
    query: str,
) -> tuple[str, list[str]]:
    """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ —Å –≤–æ–∑–≤—Ä–∞—Ç–æ–º —à—É—Ç–∫–∏ –∏ —Å—É–º–º–∞—Ä–∏–∑–∏—Ä–æ–≤–∞–Ω–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π."""
    split_documents = splitter.create_documents([new.text for new in news])
    db = FAISS.from_documents(split_documents, embedder)
    retriever = db.as_retriever()

    relevant_docs = retriever.invoke(query)
    news_summaries = summarize_news(relevant_docs, summarize_chain)

    chain = (
        {"context": lambda x: format_docs(relevant_docs), "question": RunnablePassthrough()}
        | PROMPT
        | llm
        | StrOutputParser()
    )

    joke = chain.invoke(query)

    return joke, news_summaries


def main():
    """Main application entry point."""
    load_dotenv()
    st.title("ü§ñ AI-–∞–≥–µ–Ω—Ç: –ì–µ–Ω–µ—Ä–∞—Ç–æ—Ä –º–µ–º–æ–≤ –ø–æ –ø–æ—Å–ª–µ–¥–Ω–∏–º –Ω–æ–≤–æ—Å—Ç—è–º")
    st.markdown("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –Ω–∞ –ª—é–±–æ–º —è–∑—ã–∫–µ")

    config = load_config()
    logger = setup_logging(config.logging)

    text_embedder = HFEmbedder(
        model_name=config.text_embedder,
        model_kwargs={"device": torch.device("cuda" if torch.cuda.is_available() else "cpu")},
    )
    db_manager = DatabaseManager(logger)

    classifier = TopicClassifier(
        topics=set(config.classifier.topics),
        example_articles=EXAMPLES_CLS_TOPIC,
        model_name=config.ollama.model,
        temperature=config.classifier.temperature,
        max_attempts=config.classifier.max_attempts,
        logger=logger,
    )

    splitter = CharacterTextSplitter(
        separator="\n\n",
        chunk_size=500,
        chunk_overlap=100,
        length_function=len,
        is_separator_regex=False,
    )

    llm = OllamaLLM(model=config.generator.model, temperature=config.generator.temperature)
    summarize_chain = load_summarize_chain(llm, chain_type="map_reduce", verbose=False)

    try:
        db_manager.initialize(
            {
                "user": os.getenv("POSTGRES_USER"),
                "password": os.getenv("POSTGRES_PASS"),
                "host": os.getenv("POSTGRES_HOST"),
                "port": os.getenv("POSTGRES_PORT"),
                "database": os.getenv("POSTGRES_DB"),
            }
        )

        user_query = st.text_input("–û —á–µ–º –≤—ã —Ö–æ—Ç–∏—Ç–µ –º–µ–º/–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π?", "")

        if user_query:
            with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∏ –∏—â–µ–º –ø–æ—Ö–æ–∂–∏–µ –Ω–æ–≤–æ—Å—Ç–∏..."):
                topic = classifier.classify(user_query)
                news = db_manager.get_last_news_by_topic(topic)
                joke, news_summaries = generate_response(
                    llm, news, text_embedder, summarize_chain, splitter, user_query
                )

                st.subheader(f"üì∞ –ù–æ–≤–æ—Å—Ç–∏ –ø–æ —Ç–µ–º–µ '{topic}':")
                for summary in news_summaries:
                    st.write(summary)

                st.markdown("\nüí¨ **–ú–µ–º:**")
                st.success(joke)

    except Exception as e:
        logger.error("Application error: %s", str(e), exc_info=True)
        st.error("–ü—Ä–æ–∏–∑–æ—à–ª–∞ –æ—à–∏–±–∫–∞ –ø—Ä–∏ –æ–±—Ä–∞–±–æ—Ç–∫–µ –∑–∞–ø—Ä–æ—Å–∞")

    finally:
        db_manager.close()


if __name__ == "__main__":
    main()

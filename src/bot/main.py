import os
import textwrap
from typing import Dict, List, Optional

import requests
import streamlit as st
from bs4 import BeautifulSoup
from dotenv import load_dotenv
from langchain_community.llms import Ollama
from langchain_core.output_parsers import StrOutputParser
from langchain_core.prompts import ChatPromptTemplate
from PIL import Image, ImageDraw, ImageFont

# –ó–∞–≥—Ä—É–∑–∫–∞ –ø–µ—Ä–µ–º–µ–Ω–Ω—ã—Ö –æ–∫—Ä—É–∂–µ–Ω–∏—è
load_dotenv()


class NewsAIAgent:
    def __init__(self):
        self.available_topics = ["technology", "politics", "entertainment"]
        self.llm_model = "llama3.1:8b"
        self.llm = self._init_llm()

    def _init_llm(self, temperature: float = 0.7) -> Ollama:
        """–ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è —è–∑—ã–∫–æ–≤–æ–π –º–æ–¥–µ–ª–∏"""
        return Ollama(
            base_url=os.getenv("OLLAMA_BASE_URL"),
            model=self.llm_model,
            temperature=temperature,
        )

    def _classify_query_topic(self, user_query: str) -> str:
        """–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∏–µ —Ç–µ–º–∞—Ç–∏–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ (–Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º)"""
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    f"""
             Identify the main topic of the user's query from these options:
             {', '.join(self.available_topics)}.
             Return only the topic name without additional explanations.
             """,
                ),
                ("user", "Query: {query}"),
            ]
        )

        chain = prompt | self.llm | StrOutputParser()
        topic = chain.invoke({"query": user_query}).strip()
        return topic if topic in self.available_topics else "Other"

    def _get_english_news(self, topic: str, num_news: int = 3) -> List[Dict]:
        """–ü–æ–ª—É—á–µ–Ω–∏–µ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π –ø–æ –∑–∞–¥–∞–Ω–Ω–æ–π —Ç–µ–º–µ"""
        news_items = []
        try:
            url = f"https://news.google.com/rss/search?q={topic}&hl=en-US&gl=US&ceid=US:en"
            response = requests.get(url)
            soup = BeautifulSoup(response.content, "lxml-xml")

            for item in soup.find_all("item")[:num_news]:
                news_items.append(
                    {
                        "title": item.title.text,
                        "link": item.link.text,
                        "source": item.source.text if item.source else "Unknown source",
                        "topic": topic,
                    }
                )
        except Exception as e:
            st.error(f"–û—à–∏–±–∫–∞ –ø—Ä–∏ –ø–æ–ª—É—á–µ–Ω–∏–∏ –Ω–æ–≤–æ—Å—Ç–µ–π: {e}")
        return news_items

    def _generate_russian_response(self, news_item: Dict) -> Dict:
        """–ì–µ–Ω–µ—Ä–∞—Ü–∏—è –æ—Ç–≤–µ—Ç–∞ –Ω–∞ —Ä—É—Å—Å–∫–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –Ω–æ–≤–æ—Å—Ç–∏"""
        prompt = ChatPromptTemplate.from_messages(
            [
                (
                    "system",
                    textwrap.dedent(
                        """
                –¢—ã ‚Äî –∫—Ä–µ–∞—Ç–∏–≤–Ω—ã–π –≥–µ–Ω–µ—Ä–∞—Ç–æ—Ä –∫–æ–Ω—Ç–µ–Ω—Ç–∞ –¥–ª—è —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω–æ–π –∞—É–¥–∏—Ç–æ—Ä–∏–∏.
                –ù–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–≥–ª–∏–π—Å–∫–æ–π –Ω–æ–≤–æ—Å—Ç–∏ —Å–æ–∑–¥–∞–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º —è–∑—ã–∫–µ:
                1. –ú–µ–º (–≤–µ—Ä—Ö–Ω—è—è –∏ –Ω–∏–∂–Ω—è—è —Ñ—Ä–∞–∑–∞)
                2. –û—Å—Ç—Ä—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π
                3. –ö–æ—Ä–æ—Ç–∫–∏–π —Å–∞—Ä–∫–∞—Å—Ç–∏—á–Ω—ã–π –≤—ã–≤–æ–¥

                –§–æ—Ä–º–∞—Ç:
                –ú–µ–º: [–≤–µ—Ä—Ö–Ω—è—è —Ñ—Ä–∞–∑–∞] | [–Ω–∏–∂–Ω—è—è —Ñ—Ä–∞–∑–∞]
                –ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π: [—Ç–µ–∫—Å—Ç]
                –í—ã–≤–æ–¥: [—Ç–µ–∫—Å—Ç]
            """
                    ),
                ),
                ("user", "–ê–Ω–≥–ª–∏–π—Å–∫–∞—è –Ω–æ–≤–æ—Å—Ç—å: {title}\n–ò—Å—Ç–æ—á–Ω–∏–∫: {source}"),
            ]
        )

        chain = prompt | self.llm | StrOutputParser()
        result = chain.invoke({"title": news_item["title"], "source": news_item["source"]})

        # –ü–∞—Ä—Å–∏–Ω–≥ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞
        response = {"–ú–µ–º": "", "–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π": "", "–í—ã–≤–æ–¥": ""}
        current_key = None

        for line in result.split("\n"):
            if ":" in line:
                key, value = line.split(":", 1)
                key = key.strip()
                if key in response:
                    current_key = key
                    response[current_key] = value.strip()
            elif current_key:
                response[current_key] += " " + line.strip()

        return response

    def process_user_request(self, user_query: str) -> Dict:
        """–û—Å–Ω–æ–≤–Ω–æ–π –º–µ—Ç–æ–¥ –æ–±—Ä–∞–±–æ—Ç–∫–∏ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–æ–≥–æ –∑–∞–ø—Ä–æ—Å–∞"""
        # –û–ø—Ä–µ–¥–µ–ª—è–µ–º —Ç–µ–º—É –Ω–∞ –∞–Ω–≥–ª–∏–π—Å–∫–æ–º
        topic = self._classify_query_topic(user_query)
        st.info(f"–û–ø—Ä–µ–¥–µ–ª–µ–Ω–∞ —Ç–µ–º–∞: {topic}")

        # –ü–æ–ª—É—á–∞–µ–º –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏
        news_items = self._get_english_news(topic)
        if not news_items:
            return {"error": "–ù–µ —É–¥–∞–ª–æ—Å—å –Ω–∞–π—Ç–∏ –Ω–æ–≤–æ—Å—Ç–∏ –ø–æ –¥–∞–Ω–Ω–æ–π —Ç–µ–º–µ"}

        # –í—ã–±–∏—Ä–∞–µ–º –Ω–æ–≤–æ—Å—Ç—å
        selected_news = news_items[0]

        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Ä—É—Å—Å–∫–æ—è–∑—ã—á–Ω—ã–π –∫–æ–Ω—Ç–µ–Ω—Ç
        content = self._generate_russian_response(selected_news)

        return {
            "topic": topic,
            "news": selected_news,
            "content": content,
        }


def main():
    st.title("ü§ñ –ê–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–π –Ω–æ–≤–æ—Å—Ç–Ω–æ–π AI-–∞–≥–µ–Ω—Ç —Å —Ä—É—Å—Å–∫–∏–º–∏ –æ—Ç–≤–µ—Ç–∞–º–∏")
    st.markdown("–í–≤–µ–¥–∏—Ç–µ –∑–∞–ø—Ä–æ—Å –Ω–∞ –ª—é–±–æ–º —è–∑—ã–∫–µ –∏ –ø–æ–ª—É—á–∏—Ç–µ –º–µ–º/–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π –Ω–∞ —Ä—É—Å—Å–∫–æ–º –Ω–∞ –æ—Å–Ω–æ–≤–µ –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã—Ö –Ω–æ–≤–æ—Å—Ç–µ–π")

    # –ò–Ω–∏—Ü–∏–∞–ª–∏–∑–∞—Ü–∏—è –∞–≥–µ–Ω—Ç–∞
    agent = NewsAIAgent()

    # –ü–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—å—Å–∫–∏–π –≤–≤–æ–¥
    user_query = st.text_input("–û —á–µ–º –≤—ã —Ö–æ—Ç–∏—Ç–µ –º–µ–º/–∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π?", "")

    if user_query:
        with st.spinner("–ê–Ω–∞–ª–∏–∑–∏—Ä—É–µ–º –∑–∞–ø—Ä–æ—Å –∏ –∏—â–µ–º –∞–Ω–≥–ª–æ—è–∑—ã—á–Ω—ã–µ –Ω–æ–≤–æ—Å—Ç–∏..."):
            result = agent.process_user_request(user_query)

            if "error" in result:
                st.error(result["error"])
                return

            # –û—Ç–æ–±—Ä–∞–∂–µ–Ω–∏–µ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤
            st.subheader(f"–ê–Ω–≥–ª–∏–π—Å–∫–∞—è –Ω–æ–≤–æ—Å—Ç—å –ø–æ —Ç–µ–º–µ '{result['topic']}':")
            st.markdown(f"**{result['news']['title']}**")
            st.caption(f"–ò—Å—Ç–æ—á–Ω–∏–∫: {result['news']['source']}")

            # –†–∞–∑–¥–µ–ª–µ–Ω–∏–µ –Ω–∞ –¥–≤–µ –∫–æ–ª–æ–Ω–∫–∏
            col2 = st.columns(1)

            with col2:
                st.subheader("–ê–Ω–∞–ª–∏–∑ –Ω–∞ —Ä—É—Å—Å–∫–æ–º")
                if result["content"]["–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π"]:
                    st.markdown("üí¨ **–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:**")
                    st.write(result["content"]["–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π"])

                if result["content"]["–í—ã–≤–æ–¥"]:
                    st.markdown("üîç **–í—ã–≤–æ–¥:**")
                    st.write(result["content"]["–í—ã–≤–æ–¥"])

            # –ö–Ω–æ–ø–∫–∞ –¥–ª—è –≥–µ–Ω–µ—Ä–∞—Ü–∏–∏ –Ω–æ–≤–æ–≥–æ –∫–æ–Ω—Ç–µ–Ω—Ç–∞
            if st.button("–°–≥–µ–Ω–µ—Ä–∏—Ä–æ–≤–∞—Ç—å –¥—Ä—É–≥–æ–π –≤–∞—Ä–∏–∞–Ω—Ç"):
                with st.spinner("–°–æ–∑–¥–∞–µ–º –Ω–æ–≤—ã–π –≤–∞—Ä–∏–∞–Ω—Ç..."):
                    new_content = agent._generate_russian_response(result["news"])
                    if "|" in new_content["–ú–µ–º"]:
                        st.image(agent._create_meme_image(*new_content["–ú–µ–º"].split("|")[:2]))
                    st.write("üí¨ **–ù–æ–≤—ã–π –∫–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π:**", new_content["–ö–æ–º–º–µ–Ω—Ç–∞—Ä–∏–π"])
                    st.write("üîç **–ù–æ–≤—ã–π –≤—ã–≤–æ–¥:**", new_content["–í—ã–≤–æ–¥"])


if __name__ == "__main__":
    main()
